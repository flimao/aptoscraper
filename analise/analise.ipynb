{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Análise de ofertas de imóveis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos criar uma base limpa para posterior análise no Power BI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from pandas.core.common import flatten  # para extrair entradas unicas de 'amenidades'\r\n",
    "import os\r\n",
    "import glob\r\n",
    "\r\n",
    "import geopy\r\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configurações\r\n",
    "\r\n",
    "Variáveis de configuração"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BDDIR = r'../bd'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Criação dos *dataframes* iniciais\r\n",
    "\r\n",
    "Importar os **dataframes** de cada arquivo `.csv` da base de dados e juntar esses *dataframes*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# telefones são strings!\r\n",
    "cols_tels = ['contato_fones', 'contato_whatsapp']\r\n",
    "dtypes = { tel: str for tel in cols_tels }\r\n",
    "dates_cols = ['atualizado_em']\r\n",
    "\r\n",
    "# carregar todos os CSV na pasta relevante que começam por 'zap_'\r\n",
    "\r\n",
    "mascara_parcial = r'zap_*.csv'\r\n",
    "mascara = os.path.join(os.path.abspath(BDDIR), mascara_parcial)\r\n",
    "\r\n",
    "bd_csvs = glob.glob(mascara)\r\n",
    "bd_lstdfs = [ pd.read_csv(bd_csv, dtype = dtypes, parse_dates = dates_cols) for bd_csv in bd_csvs ]\r\n",
    "\r\n",
    "portaldf_raw = pd.concat(bd_lstdfs, axis = 0)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pré-processamento - Limpeza\r\n",
    "\r\n",
    "Vamos criar um *pipeline* para limpar o *dataframe* principal (`portaldf_raw`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *DataFrame* de análise de variação dos preços ao longo do tempo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1. determinando indice\r\n",
    "def setar_indice(df, indice = ['origem', 'id']):\r\n",
    "    df = df.copy()\r\n",
    "    df.set_index(keys = indice, drop = True, inplace = True)\r\n",
    "    return df\r\n",
    "\r\n",
    "# 2. eliminar duplicatas\r\n",
    "def eliminar_duplicatas(df, subset = ['atualizado_em']):\r\n",
    "    index_cols = list(df.index.names)\r\n",
    "    subset_real = index_cols + subset\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    # não conseguimos dropar duplicatas baseados no índice\r\n",
    "    df2 = df.reset_index().drop_duplicates(subset = subset_real)\r\n",
    "\r\n",
    "    # retornamos ao indice original\r\n",
    "    df3 = df2.set_index(index_cols, drop = True)\r\n",
    "\r\n",
    "    return df3\r\n",
    "\r\n",
    "# 3. adicionar data de atualização ao índice (de forma a garantir que seja único)\r\n",
    "def add_indice(df, cols = ['atualizado_em']):\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    df2 = df.set_index(cols, append = True, drop = True)\r\n",
    "\r\n",
    "    return df2\r\n",
    "\r\n",
    "# 4. fillna\r\n",
    "def preencher_vazios(df, cols_fillna = ['nsuites', 'nvagas'], astype = np.int8):\r\n",
    "    df = df.copy()\r\n",
    "    df[cols_fillna] = df[cols_fillna].fillna(0)\r\n",
    "    df[cols_fillna] = df[cols_fillna].astype(astype)\r\n",
    "    return df\r\n",
    "\r\n",
    "# 5. preencher lat e long faltantes\r\n",
    "def preencher_latlong(df):\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    # funcoes para construcao de queries\r\n",
    "\r\n",
    "    def construir_dict_rua(df_ends):\r\n",
    "        query_dict = df_ends.apply(lambda linha: {\r\n",
    "            'street': linha['endereco_rua'],\r\n",
    "            'city': linha['endereco_cidade'],\r\n",
    "            'state': linha['endereco_estado'],\r\n",
    "            'country': linha['endereco_pais']\r\n",
    "        }, axis = 1)\r\n",
    "        return query_dict\r\n",
    "\r\n",
    "    def construir_dict_cep(df_ends):\r\n",
    "        query_dict = df_ends.apply(lambda linha: {\r\n",
    "            'postalcode': linha['endereco_cep'],\r\n",
    "            'country': linha['endereco_pais']\r\n",
    "        }, axis = 1)\r\n",
    "\r\n",
    "        return query_dict\r\n",
    "    \r\n",
    "    # filtrando o df para só incluir entradas com latitude e longitude nulas\r\n",
    "    df_ends_full = df[[c for c in df if c.startswith('endereco')]].copy()\r\n",
    "    df_ends_dbl = df_ends_full.loc[df_ends_full.endereco_latitude.isna()]\r\n",
    "\r\n",
    "    # preencher as entradas faltantes com base no CEP:\r\n",
    "    df_ends_dbl2 = df_ends_dbl.groupby('endereco_cep').transform('first')\r\n",
    "    df_ends_dbl2['endereco_cep'] = df_ends_dbl['endereco_cep']\r\n",
    "\r\n",
    "    # deduplicar com a coluna CEP. depois preencheremos as duplicatas\r\n",
    "    # o objetivo é minimizar as chamadas de API\r\n",
    "    df_ends = df_ends_dbl2.drop_duplicates(\r\n",
    "        subset = ['endereco_pais', 'endereco_estado', 'endereco_cidade', 'endereco_rua'],\r\n",
    "    )\r\n",
    "\r\n",
    "    # construir a query com base nos enderecos\r\n",
    "    # caso não exista alguma informação, construir a query com base no cep\r\n",
    "\r\n",
    "    queries = construir_dict_rua(df_ends).where(   # endereco completo...\r\n",
    "                    df_ends.endereco_rua.notna(),  # ...onde a rua estiver preenchida...\r\n",
    "                    construir_dict_cep(df_ends))   # .. caso contrario CEP\r\n",
    "\r\n",
    "    # construir a função geocode\r\n",
    "    geocoder = geopy.Nominatim(user_agent = 'imoveis-bot', timeout = 5)\r\n",
    "    geocode_rl = RateLimiter(geocoder.geocode, min_delay_seconds = 1)\r\n",
    "\r\n",
    "    # Series com locator com longitude e latitude\r\n",
    "    locators = queries.apply(geocode_rl)\r\n",
    "\r\n",
    "    # extrair latitude e longitude\r\n",
    "    latlong = (locators\r\n",
    "        .apply(lambda l: {\r\n",
    "            'endereco_latitude': l.latitude,\r\n",
    "            'endereco_longitude': l.longitude\r\n",
    "        } if l is not None else {'endereco_latitude': np.nan, 'endereco_longitude': np.nan})\r\n",
    "        .apply(pd.Series)  # para transformar em um DataFrame\r\n",
    "    )\r\n",
    "\r\n",
    "    # juntar as lats e longs obtidas no df principal\r\n",
    "    # após essa operação, vai faltar repetir essas lats e longs para as entradas\r\n",
    "    # repetidas que removemos\r\n",
    "    latlong_full_na = df_ends_dbl2[latlong.columns].where(df_ends_dbl2[latlong.columns].notna(), latlong)\r\n",
    "    df_ends_dbl2[latlong.columns] = latlong_full_na\r\n",
    "\r\n",
    "    # repetindo para entradas repetidas que tiramos anteriormente\r\n",
    "    df_ends_final = df_ends_dbl2.copy()\r\n",
    "    df_ends_final[latlong.columns] = df_ends_dbl2.groupby('endereco_cep')[latlong.columns].transform('first')\r\n",
    "\r\n",
    "    # integrando o df com enderecos ao df principal\r\n",
    "    latlong_final = df[latlong.columns].where(df[latlong.columns].notna(), df_ends_final[latlong.columns])\r\n",
    "    df[latlong.columns] = latlong_final\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "# (CORTADO DO PIPELINE) dropar coluna de amenidades do df principal\r\n",
    "def dropar_cols(df, cols):\r\n",
    "    df = df.copy()\r\n",
    "    df = df.drop(columns = cols)\r\n",
    "    return df\r\n",
    "\r\n",
    "# n-2. eliminar colunas\r\n",
    "def escolher_colunas(df, \r\n",
    "            cols_manter = [\r\n",
    "                'endereco_bairro', 'endereco_rua', 'endereco_complemento',\r\n",
    "                'endereco_latitude', 'endereco_longitude',\r\n",
    "                'area', 'desc',\r\n",
    "                'nquartos', 'nbanheiros', 'nsuites', 'nvagas',\r\n",
    "                'metro_trem', 'onibus', 'cafes',\r\n",
    "                'preco', 'iptu', 'despesa_mes', 'despesa_ano',\r\n",
    "                'link'\r\n",
    "            ]\r\n",
    "        ):\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    df = df.drop(columns = set(df.columns) - set(cols_manter))\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "# n-1. associar amenidades\r\n",
    "def associar_amenidades(df, df_amenidades, \r\n",
    "                        amenidades_cols = ['PLAYGROUND', 'BALCONY', 'CLOSET']):\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    df = df.join(df_amenidades[amenidades_cols], how = 'left')\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "# n. ordenar linhas\r\n",
    "def sort_linhas(df, key = ['area', 'nsuites', 'nquartos', 'preco', 'iptu']):\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    df = df.sort_values(key, ascending = False)\r\n",
    "\r\n",
    "    return df\r\n",
    "\r\n",
    "# extra: one-hot encoding de colunas com csv\r\n",
    "def extrair_ohe_csv(df, col, sep = ','):\r\n",
    "    df = df.copy()\r\n",
    "\r\n",
    "    try:\r\n",
    "        col_gen = flatten(df[col].str.split(sep))\r\n",
    "    except AttributeError: # coluna não tem nenhuma string. Vamos retornar um DF vazio\r\n",
    "        return pd.DataFrame([], index = df.index)\r\n",
    "    \r\n",
    "    col_set = set(col_gen)\r\n",
    "    col_set.discard(np.nan)\r\n",
    "    coldf = pd.DataFrame({\r\n",
    "        colitem: (df[col]\r\n",
    "                    .str.contains(colitem)\r\n",
    "                    .where(df[col].notna(), False)\r\n",
    "                ) for colitem in col_set})\r\n",
    "    \r\n",
    "    return coldf\r\n",
    "\r\n",
    "# executar pipeline\r\n",
    "\r\n",
    "# processamento de pontos de interesse\r\n",
    "# detecção dos tipos\r\n",
    "\r\n",
    "portaldf_indice = (portaldf_raw\r\n",
    "    .pipe(setar_indice)\r\n",
    "    .pipe(eliminar_duplicatas)\r\n",
    "    .pipe(add_indice, cols = ['atualizado_em'])\r\n",
    ")\r\n",
    "\r\n",
    "onibus = extrair_ohe_csv(portaldf_indice, col = 'onibus')\r\n",
    "metro_trem = extrair_ohe_csv(portaldf_indice, col = 'metro_trem')\r\n",
    "farmacias = extrair_ohe_csv(portaldf_indice, col = 'farmacias')\r\n",
    "pois = extrair_ohe_csv(portaldf_indice, col = 'pois')\r\n",
    "\r\n",
    "# processamento de amenidades\r\n",
    "amenidades = extrair_ohe_csv(portaldf_indice, col = 'amenidades')\r\n",
    "\r\n",
    "portaldf_timeseries = (portaldf_indice\r\n",
    "    .pipe(preencher_vazios)\r\n",
    "    .pipe(preencher_latlong)\r\n",
    "    .pipe(escolher_colunas)\r\n",
    "    .pipe(associar_amenidades, df_amenidades = amenidades)\r\n",
    "    .pipe(sort_linhas)\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *DataFrame* de análise de preços mais recentes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "atualizacao_mais_recente = (portaldf_timeseries\r\n",
    "    .reset_index()\r\n",
    "    .groupby(['origem', 'id'])['atualizado_em']\r\n",
    "    .transform(lambda dt_atualizacao: dt_atualizacao == dt_atualizacao.max())\r\n",
    ") \r\n",
    "atualizacao_mais_recente.index = portaldf_timeseries.index\r\n",
    "portaldf = portaldf_timeseries[atualizacao_mais_recente].copy()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'portaldf_timeseries' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9259923a618f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m atualizacao_mais_recente = (portaldf_timeseries\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'origem'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'atualizado_em'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mdt_atualizacao\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdt_atualizacao\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdt_atualizacao\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'portaldf_timeseries' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exportar as bases limpas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "csv_salvar_fn_timeseries = os.path.join(os.path.abspath(BDDIR), r'processado', r'imoveis_timeseries.csv')\r\n",
    "csv_salvar_fn_listagens = os.path.join(os.path.abspath(BDDIR), r'processado', r'imoveis.csv')\r\n",
    "portaldf_timeseries.to_csv(csv_salvar_fn_timeseries)\r\n",
    "portaldf.to_csv(csv_salvar_fn_listagens)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('aptoscraper': conda)"
  },
  "interpreter": {
   "hash": "c828b8cb6de1fa072523b46f37692a01a18c8521b34d59761e48bd2df276b786"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}